# Robo_ND_Perception_Project

This is the third project of the Robotics Software Engineering Nanodegree. The goal is to create a perception pipeline that is able to identify objects on a table top setting. An image of a table-top full of objects to be classified is filtered by the following filters: Voxel grid downsampling, pass through filter, RANSAC. Next, an object segmentation algorithm, DBSCAN, is used to separate the objects into different colored clusters. Finally, the image is run through a trained SVM classifier to classify the objects in the table-top scene. The code in project_template.py implements these steps to classify 3 different table-top scenes depending on which scene is given as input. The code in capture_features.py is run with Gazebo's simulation of objects radomly spawned in different orientations to extract features from different samples of our objects to be classified and saves it in a training_set.sav file. The features examined here are the normals of the objects and the HSV. The code in train_svm.py then takes the information gathered about our samples to creates an SVM classifier with sigmoid kernel and saves this classifier in a model.sav file. The integers on the .sav files indicate one of 3 table-top worlds we are considering, each with different objects. After project_template.py is run, the result of the classification is stored in a .yaml file which includes the centroid of each identified object for the further task of the PR2 robot picking and placing the object into a side bin. The results of object classification for this project for each respective world are: World 1: 3/3 World 2: 5/5 World 3: 8/8. 
